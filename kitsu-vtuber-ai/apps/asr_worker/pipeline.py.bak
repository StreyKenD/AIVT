from __future__ import annotations

import asyncio
import os
import time
from typing import AsyncIterator, Optional

from .config import ASRConfig
from .logger import logger
from .metrics import ASRTelemetry
from .orchestrator import OrchestratorPublisher
from .transcription import Transcriber
from .vad import VoiceActivityDetector

_ALLOW_NON_ENGLISH = os.getenv("ASR_ALLOW_NON_ENGLISH", "0").lower() in {
    "1",
    "true",
    "yes",
}
if _ALLOW_NON_ENGLISH:
    logger.warning("ASR language filter disabled via ASR_ALLOW_NON_ENGLISH")

class SpeechPipeline:
    def __init__(
        self,
        *,
        config: ASRConfig,
        vad: VoiceActivityDetector,
        transcriber: Transcriber,
        orchestrator: OrchestratorPublisher,
        telemetry: ASRTelemetry | None = None,
    ) -> None:
        self._config = config
        self._vad = vad
        self._transcriber = transcriber
        self._orchestrator = orchestrator
        self._telemetry = telemetry or ASRTelemetry(None)
        self._speech_active = False
        self._buffer = bytearray()
        self._silence_frames = 0
        self._segment_started_at = 0.0
        self._last_partial_at = 0.0
        self._last_partial_text = ""
        self._segment_counter = 0

    @staticmethod
    def _language_is_english(language: Optional[str]) -> bool:
        if _ALLOW_NON_ENGLISH:
            return True
        if not language:
            return False
        normalized = language.strip().lower()
        if not normalized:
            return False
        if normalized in {"und", "xx"}:
            return True
        return normalized == "en" or normalized.startswith("en-")

    async def process(self, frames: AsyncIterator[bytes]) -> None:
        logger.debug("Speech pipeline consuming audio frames")
        try:
            async for frame in frames:
                await self._handle_frame(frame)
        except asyncio.CancelledError:
            raise
        logger.warning(
            "Speech pipeline frames iterator completed unexpectedly (speech_active=%s)",
            self._speech_active,
        )

    async def _handle_frame(self, frame: bytes) -> None:
        if len(frame) != self._config.frame_bytes:
            logger.debug("Adjusting frame size mismatch: got %d bytes", len(frame))
            frame = frame[: self._config.frame_bytes]
            if len(frame) < self._config.frame_bytes:
                frame = frame.ljust(self._config.frame_bytes, b"\x00")
        if self._vad.is_speech(frame):
            await self._on_speech(frame)
        else:
            await self._on_silence()

    async def _on_speech(self, frame: bytes) -> None:
        if not self._speech_active:
            self._speech_active = True
            self._buffer.clear()
            self._segment_started_at = time.time()
            self._last_partial_at = 0.0
            self._last_partial_text = ""
            self._segment_counter += 1
            logger.debug("Speech segment %s started", self._segment_counter)
        self._buffer.extend(frame)
        self._silence_frames = 0
        now = time.time()
        if now - self._segment_started_at < 0.1:
            return
        if (
            self._last_partial_at
            and now - self._last_partial_at < self._config.partial_interval
        ):
            return
        await self._emit_partial(now)

    async def _on_silence(self) -> None:
        if not self._speech_active:
            return
        self._silence_frames += 1
        if self._silence_frames < self._config.silence_threshold_frames:
            return
        ended_at = time.time()
        await self._emit_final(ended_at)
        self._speech_active = False
        self._buffer.clear()
        self._silence_frames = 0

    async def _emit_partial(self, timestamp: float) -> None:
        text, confidence, language = await self._transcribe_async()
        if not text:
            return
        if not self._language_is_english(language):
            logger.info(
                "Skipping ASR partial for non-English language '%s' (segment=%s)",
                language,
                self._segment_counter,
            )
            await self._telemetry.segment_skipped(
                segment=self._segment_counter,
                reason="non_english_partial",
                language=language,
                text_length=len(text),
            )
            self._last_partial_text = text
            self._last_partial_at = timestamp
            return
        if text == self._last_partial_text:
            return
        self._last_partial_text = text
        self._last_partial_at = timestamp
        latency_ms = (timestamp - self._segment_started_at) * 1000
        payload = {
            "segment": self._segment_counter,
            "text": text,
            "confidence": confidence,
            "language": language,
            "started_at": self._segment_started_at,
            "ended_at": timestamp,
            "latency_ms": latency_ms,
        }
        logger.info("[ASR] partial (%.0f ms): %s", latency_ms, text)
        await self._orchestrator.publish("asr_partial", payload)
        await self._telemetry.segment_partial(
            segment=self._segment_counter,
            latency_ms=latency_ms,
            text=text,
            confidence=confidence,
            language=language,
        )

    async def _emit_final(self, timestamp: float) -> None:
        text, confidence, language = await self._transcribe_async()
        if not text:
            logger.debug("Discarding empty final segment")
            return
        text_length = len(text)
        if not self._language_is_english(language):
            logger.info(
                "Dropping ASR final for non-English language '%s' (segment=%s)",
                language,
                self._segment_counter,
            )
            await self._telemetry.segment_skipped(
                segment=self._segment_counter,
                reason="non_english_final",
                language=language,
                text_length=text_length,
            )
            return
        duration_ms = (len(self._buffer) / 2 / self._config.sample_rate) * 1000
        payload = {
            "segment": self._segment_counter,
            "text": text,
            "confidence": confidence,
            "language": language,
            "started_at": self._segment_started_at,
            "ended_at": timestamp,
            "duration_ms": duration_ms,
        }
        logger.info(
            "[ASR] final (%.0f ms, conf=%s): %s",
            duration_ms,
            f"{confidence:.2f}" if confidence is not None else "n/a",
            text,
        )
        await self._orchestrator.publish("asr_final", payload)
        await self._telemetry.segment_final(
            segment=self._segment_counter,
            duration_ms=duration_ms,
            confidence=confidence,
            language=language,
            text=text,
        )

    async def _transcribe_async(self) -> tuple[str, Optional[float], Optional[str]]:
        loop = asyncio.get_running_loop()
        audio = bytes(self._buffer)
        result = await loop.run_in_executor(None, self._transcriber.transcribe, audio)
        return result.text, result.confidence, result.language


__all__ = ["SpeechPipeline"]
